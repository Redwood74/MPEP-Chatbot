{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guinness beer, the iconic Irish stout, is typically a dark brown or black in color. This deep hue comes from its high concentration of melanoidins, which are created during the brewing process when malted barley and roasted unmalted barley (called \"black patent\") are used in the beer's recipe.\n",
      "\n",
      "However, it is worth noting that there may also be a lighter variety of Guinness called \"Guinness Foreign Extra Stout\" which has a slightly different color and appearance compared to the traditional dark stout. This variation uses roasted barley in its brewing process but at a lower level than the mainstream version, resulting in a less intense black-brown hue.\n"
     ]
    }
   ],
   "source": [
    "#test the ollama local setup\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"phi3\", request_timeout=60.0)\n",
    "\n",
    "response = llm.complete(\"What color is Guinness beer?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test olamma embedding\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"phi3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "\n",
    "#uncomment to test\n",
    "# pass_embedding = ollama_embedding.get_text_embedding_batch(\n",
    "#     [\"This is a passage!\", \"This is another passage\"], show_progress=True\n",
    "# )\n",
    "# print(pass_embedding)\n",
    "\n",
    "# query_embedding = ollama_embedding.get_query_embedding(\"Where is blue?\")\n",
    "# print(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def to_langchain_format(chapter):\n",
    "    \"\"\"\n",
    "    Transforms a chapter dictionary into a standardized format expected by LangChain.\n",
    "\n",
    "    Args:\n",
    "        chapter (dict): A dictionary containing the keys 'chapter_title' and 'content'\n",
    "                        which hold the title and content of the chapter, respectively.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary formatted for LangChain, with the title and cleaned content.\n",
    "    \"\"\"\n",
    "    # Assuming 'content' is already cleaned\n",
    "    return {\n",
    "        \"title\": chapter[\"chapter_title\"],\n",
    "        \"content\": chapter[\"content\"]  # already cleaned\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data ready for LangChain saved to: C:\\Users\\mharr\\OneDrive\\Documents\\GitHub\\MPEP_finetune\\Notebooks\\langchain_ready_data.json\n"
     ]
    }
   ],
   "source": [
    "#data transformation for langchain format\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the cleaned data\n",
    "file_path = r'C:\\Users\\mharr\\OneDrive\\Documents\\GitHub\\MPEP_finetune\\Notebooks\\mpep_data_clean.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    cleaned_chapters = json.load(file)\n",
    "\n",
    "# Apply the transformation to each chapter\n",
    "processed_documents = [to_langchain_format(chapter) for chapter in cleaned_chapters]\n",
    "\n",
    "# Optionally, save the processed documents to a new file\n",
    "output_path = r'C:\\Users\\mharr\\OneDrive\\Documents\\GitHub\\MPEP_finetune\\Notebooks\\langchain_ready_data.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(processed_documents, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Processed data ready for LangChain saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain documents object\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=r\"C:\\Users\\mharr\\OneDrive\\Documents\\GitHub\\MPEP_finetune\\Notebooks\\langchainjson\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:20<00:00, 20.51s/it]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [01:12<00:00, 28.36it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [01:57<00:00, 17.44it/s]\n",
      "Generating embeddings: 100%|██████████| 829/829 [00:39<00:00, 21.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#embeddings creation\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=ollama_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=ollama_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The utility requirement of a patent refers to the fundamental criterion that an invention must have a specific and credible utility or use. In other words, for an invention to be eligible for a patent, it must serve a practical purpose and be capable of providing some form of benefit or advantage. This requirement ensures that patents are granted for inventions that are actually useful and have real-world applications. Additionally, the utility of the invention must be described in the patent application in a manner that enables a person skilled in the relevant field to understand and appreciate the practical significance of the invention.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Data from the persisted index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are the utility requirements of a patent? Please explain in detail.\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpepenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
